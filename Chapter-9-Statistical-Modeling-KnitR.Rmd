---
title: "Ch. 9 Statistical Modeling and KnitR"
author: "Rose An, Michael Krzywicki"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: TRUE
    toc_float: TRUE
link-citations: yes
fontsize: 12pt
bibliography: Bibliography_Reproducible_Science_9.bib
csl: AmJBot.csl
---
# Introduction
In this tutorial, we will learn how to set up a R Markdown document for reproducible statistical analysis/modeling, primarily using functions and syntax from the KnitR package. On Tuesday, we will ensure that everyone has downloaded the right data and packages, go over code chunk setup, and go over different ways to source/code analysis. On Thursday, we will learn how to ensure analysis is reproducibly random and how to efficiently run computationally-intensive analysis. 

## Aim
We will learn how to dynamically connect data gathering and source code to markdown documents, setup pipelines to rerun analysis and present results whenever compiled, and keep markdown documents up to date and reflecting changes made to the data or analysis in source code files.

## Why is it important?
Coding is hard. It takes a long time, and you'll likely have to go back to the same analysis multiple times to tweak and change things. It is important to setup a good workflow early to ensure you're not scrambling to make your research reproducible at the end of the process. You'll be saving yourself a lot of time if your analysis is reproducible and your results presented every time you knit your document.

# Learning Outcomes
Through this tutorial, students will learn to:

1. Analyze data in a code chunk reproducibly.
2. Dynamically report variables in text.
3. Source or code analysis through multiple methods.
4. Ensure results are reproducibly random.
5. Run computationally intensive analysis efficiently.


# Packages
Make sure you have the following packages: "knitr", "rmarkdown", "bookdown", "formattable", "kableExtra", "dplyr", "magrittr", "prettydoc", "htmltools", "knitcitations", "bibtex", "devtools", "tidyverse", "ebirdst", "sf", "raster", "terra", "patchwork", "tigris", "rnaturalearth", "ggplot2". Below are the code chunks for loading packages and setting up the document from Ch. 2:


```{r packages, results = 'hide', message = FALSE, warning = FALSE}
###~~~
# Load R packages
###~~~
#Create a vector w/ the required R packages
# --> If you have a new dependency, don't forget to add it in this vector
pkg <- c("knitr", "rmarkdown", "bookdown", "formattable", "kableExtra", "dplyr", "magrittr", "prettydoc", "htmltools", "knitcitations", "bibtex", "devtools", "tidyverse", "ebirdst", "sf", "raster", "terra", "patchwork", "tigris", "rnaturalearth", "ggplot2")

##~~~
#2. Check if pkg are already installed on your computer
##~~~
print("Check if packages are installed")
#This line below outputs a list of packages that are not installed. Which ones of the packages above are installed in computer, print packages that are not installed. Should print 0.
new.pkg <- pkg[!(pkg %in% installed.packages())]

##~~~
#3. Install missing packages
##~~~
# Use an if/else statement to check whether packages have to be installed
# WARNING: If your target R package is not deposited on CRAN then you need to adjust code/function
if(length(new.pkg) > 0){
  print(paste("Install missing package(s):", new.pkg, sep=' '))
  install.packages(new.pkg, dependencies = TRUE)
}else{
  print("All packages are already installed!")
}

##~~~
#4. Load all required packages
##~~~
print("Load packages and return status")
#Here we use the sapply() function to require all the packages
# To know more about the function type ?sapply() in R console
# Just an easier way to load all packages
sapply(pkg, require, character.only = TRUE)
# debug = right-pointing arrow next to code will load in console so you can go line by line and debug. pressing this also seemed to resolve the CRAN needs a mirror error
# Generate BibTex citation file for all loaded R packages
# used to produce report Notice the syntax used here to
# call the function
knitr::write_bib(.packages(), file = "packages.bib")
# The .packages() function returns invisibly the names of all packages loaded in the current R session (to see the output, use .packages(all.available = TRUE)). This ensures that all packages used in your code will have their citation entries written to the .bib file. This packages is then used in the Appendix to cite the code used
```

Setup Chunk Options:
```{r setup, results = 'hide', cache = FALSE}
### Chunk options: see http://yihui.name/knitr/options/ ###
### Text output
opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE, include = TRUE)
## Code formatting
opts_chunk$set(tidy = TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60),
    highlight = TRUE)
## Code caching
opts_chunk$set(cache = 2, cache.path = "cache/")
## Plot output The first dev is the master for the output
## document
opts_chunk$set(fig.path = "Figures_MS/", dev = c("png", "pdf"),
    dpi = 300)
## Figure positioning
opts_chunk$set(fig.pos = "H")
```

# Data
Download data from google drive. For this chapter, we will be working with a couple of datasets. Mike is working on consolidating some different exploratory analyses he did at different times over the last year. He wants to get them in the same place and make sure that they are reproducible. One set of data comes from the supplementary material of a paper. It is a csv file that was pulled from one of the author of the paper's github. The other dataset came from the r package `ebirdst`, which interfaces with data from the community science platform eBird. We'll start with a dataset of "encounter rates" calculated from eBird data, which are calculated as the percentage of checklists that include a particular species in a given area [@Schuetz_and_Johnston_2019]. This data has been analyzed for the United States by state, and Mike is getting ready to calculate encounter rates for species in Sonora, Mexico that are also found in Idaho. Before digging into the eBird data for Sonora, Mike wanted to get to know the dataset for Idaho so that he knows what species to look for in the Sonora data. Let's get that data ready. 

#Load Data 

```{r}
library(tidyverse)

#load csv file of data
state_enc_rate <- read.csv("01_raw_data/state.level.enc.rate.csv")

summary(state_enc_rate)
head(state_enc_rate)

```

There is more data in this dataframe than we need, so lets do some transformation to get the data where we want it. Specifically, we are going to drop a column we don't need, filter the data down to just Idaho, and filter for species with encounter rates higher than 50%. 

```{r, warning = FALSE}
#Get rid of uneccessary columns
state_enc_rate_clean <- state_enc_rate %>%
  dplyr::select(
    -query.volume.state #not needed for analysis
  )

view(state_enc_rate_clean)

#filter to just Idaho
state_enc_rate_clean_filtered <- state_enc_rate_clean %>%
  filter(
    state == "Idaho",
    
  )

view(state_enc_rate_clean_filtered)

#get rid of species with no encounter rate
ID_enc_rate <- state_enc_rate_clean_filtered %>%
  filter(encounter.state.normalized!=0
  )

#get ride of encounter rates below 50 to focus on commonly encou
ID_enc_rate <- ID_enc_rate %>%
  filter(encounter.state.normalized > 50)

#check size of final dataframe
summary(ID_enc_rate)

#make a table for markdown output
kable(ID_enc_rate)

#create clean data folder and save new data file
dir.create("02_Clean_data", showWarnings = FALSE)
write.csv(ID_enc_rate, file = "02_Clean_data/ID_enc_rate.csv",
    row.names = FALSE)

 
```


# Analyze data in a code chunk reproducibly.
Although dynamically linking source code to markdown documents is the main goal of this chapter, sometimes it makes sense to include short code chunks directly into your markdown document. There are a variety of customizable options on how the code and its output are display in the final knitted document. We use code chunk arguments from the knitr package to choose the parts of our code chunks that are outputted in the final html document.

## Set up a chunk's outputs correctly.
First, we will learn how to set up a code chunk. We use code chunk arguments from the knitr package to choose the parts of our code chunks that are outputted in the final html document.

Code chunk arguments:

+ include = FALSE hides code and results from the output document (html in our case), but code will still run
+ eval = FALSE to include code in the document without running it
+ echo = FALSE hides code but not results from the document
+ results = 'hide' hides results but not the code from the document
+ warning, message, error = FALSE hides warnings, messages, and error messages from the document

For eval and echo, you can tell knitr which expressions in the chunk to include using numerical vectors. For example, eval = 1:2 will only include the 1st 2 expressions of code in the output document.

### Thought Excercise:
Take 5 minutes to discuss with the person sitting next to you. Given the same chunk of analysis, how would you set up your chunk arguments when the document is being sent to your advisor for review versus being sent to a journal for publication? Then share with the class.

## Analysis in a code chunk
For this tutorial, we will create a code chunk to compare the mean encounter rate of songbirds and waterfowl in the Idaho dataset. 

### Challenge:
What code chunk arguments should we use if we want to figure to show up in the output document but not the code?

## Load Data 

First we are going to to do a little bit more data transformation to make a couple of objects of the Idaho encounter rate data grouped taxonomically for some descriptive statistics we will use later.

```{r}


#Pull passerines from ID encounter rates list
passerine_enc_rate <- ID_enc_rate %>%
  filter(common.name %in% c("American robin", "bank swallow", "black-billed magpie", "black-headed grosbeak", "Brewer's blackbird", "Bullock's oriole", "Cassin's finch", "Cassin's vireo", "cedar waxwing", "cliff swallow", "dark-eyed junco", "dusky flycatcher", "European starling", "evening grosbeak", "Hammond's flycatcher", "house finch", "lazuli bunting", "MacGillivray's warbler", "mountain chickadee", "northern rough-winged swallow", "olive-sided flycatcher", "pine siskin", "red crossbill", "red-winged blackbird", "sage thrasher", "song sparrow", "western kingbird", "western tanager", "western wood-pewee", "willow flycatcher", "yellow warbler"))

waterfowl_enc_rate <- ID_enc_rate %>%
  filter(common.name %in% c("American coot", "American wigeon", "cinnamon teal", "common goldeneye", "common merganser", "mallard", "ring-necked duck", "western grebe"))

```




## Identify the correct chunk commands to use in different situations.


What do the chunk commands do:

# Set up a chunk's outputs correctly.
We use code chunk arguments from the knitr package to choose the parts of our code chunks that are outputted in the final html document.



Code chunk arguments:

+ include = FALSE hides code and results from the output document (html in our case), but code will still run
+ eval = FALSE to include code in the document without running it
+ echo = FALSE hides code but not results from the document
+ results = 'hide' hides results but not the code from the document
+ warning, message, error = FALSE hides warnings, messages, and error messages from the document



### Thought Exercise:
Take 5 minutes to discuss with the person sitting next to you. Given the same chunk of analysis, how would you set up your chunk arguments when the document is being sent to your advisor for review versus being sent to a journal for publication? Then share with the class.

# Dynamically include locally sourced modular analysis

For eval and echo, you can tell knitr which expressions in the chunk to include using numerical vectors. For example, eval = 1:2 will only include the 1st 2 expressions of code in the output document.










# Dynamically report variables in your text.
Sometimes you may want to have R code or output appear inline with the rest of the markdown document. These can be static or dynamic.

* **Static**: When you want to display a line of code in the markdown document.
  * **Example**: Here is a line of code from data prep for this lesson: `ID_enc_rate <- state_enc_rate_clean_filtered %>%
  filter(encounter.state.normalized!=0
  )`

- **Dynamic**: When you want to display output of a line of code inline with the rest of the markdown document.
  - **Example**: The average encounter rate for the most commonly encountered passerines in Idaho is `r mean(passerine_enc_rate$encounter.state.normalized, na.rm = TRUE)`.


# Source or code analysis through multiple methods.
We will go over two other ways to include analysis in your R Markdown document: locally sourced modular analysis and url-sourced modular analysis. We will also go over the scenarios in which you might use each method.

## Locally sourced modular analysis
You may have previously written code locally stored on your computer that you want to pull for an analysis. To do so, we will save that code in its own R file and use the source command from knitR to call it. This is most helpful in the early stages of your research, as this is not the best practice for ensuring reproducible science. 

```{r}
# Run main analysis
source("03_linked_scripts/Sonora_AMAV_migration_chron.R")

AMAV_SO
```


Use the argument `include = FALSE` to run the analysis and produce objects that can still be used by other code chunks but will not show up in the output document.


## Url-sourced modular analysis
Once you are further along in your research, you may want to make sure your file in publicly accessible to ensure reproducibility. You can host your file in a GitHub repository and use the source_url command from the devtools package to call it.

We have set up a pre-made GitHub file for this section.


# Ensure results are reproducibly random
When an analysis produces simulations, a random number generator is generally used so that others can replicate your results. For this tutorial, we will do an exercise to learn the importance of using the set.seed command to ensure replicability.

### Exercise:
You and the person next to you use the same distribution variables but diff vs. same set.seed. Do you get the same result? What about when you use the same set.seed? 

```{r, cache=TRUE}
# Set seed as 125
set.seed(125)

# Draw 1000 numbers
Draw2 <- rnorm(1000, mean = 0, sd = 2) # Summarize Draw1

# Draw 1000 other numbers
Draw1 <- rnorm(1000, mean=5, sd=1.5)

summary(Draw2)

plot(Draw2)

plot(Draw1)

model <- lm(Draw1~Draw2)

plot(model)

# Set seed as 150
set.seed(150)

# Draw 1000 numbers
Draw3 <- rnorm(1000, mean = 0, sd = 2) # Summarize Draw1

# Draw 1000 other numbers
Draw4 <- rnorm(1000, mean=5, sd=1.5)

summary(Draw2)

plot(Draw2)

plot(Draw1)

model1 <- lm(Draw3~Draw4)

plot(model)

# Save sample
save(model, file = "model.RData")
```


# Run computationally intensive analysis efficiently.
When an analysis is too computationally intensive, knowing how to use the cache functions from knitR is helpful. Caching a code chunk will store the results of its analysis in a cache folder.

To cache a code chunk, use the argument `cache = TRUE`. This will ensure the chunk will only run if the chunk's contents or options change. 

### Excercise
Try `cache = TRUE` on the code chunk from earlier. Knit the document. What happens? Run the document again. What happens?

### Excercise
However, subsequent chunks will not be able to access objects created by the chunks or any packages loaded in it (all packages should be loaded in their own chunk anyways though), and the cached chunk will not run if a previous chunk with code it depended on changes. You can solve this last problem with the argument dependson. A chunk with the dependson argument will rerun if the specified chunk is also rerun. To specify a chunk, use a vector of their labels or their number order from the start of the document. For example, `dependson = c(2,3)` will rerun the cached chunk if the 2nd and 3rd chunks of the document are also rerun. You can also save objects created by a cached chunk to a separate RData file and load that in subsequent chunks with the load command. The last line of code in our previous chunk was to save the output of the cached chunk in an RData file. In the next chunk, we will call the output without running the last chunk:

```{r Histogram}

# Load Sample
load(file = "model.RData" )

summary(model)

plot(model)
```



However, subsequent chunks will not be able to access objects created by the chunks or any packages loaded in it (all packages should be loaded in their own chunk anyways though), and the cached chunk will not run if a previous chunk with code it depended on changes. You can solve this last problem with the argument dependson. A chunk with the dependson argument will rerun if the specified chunk is also rerun. To specify a chunk, use a vector of their labels or their number order from the start of the document. For example, `dependson = c(2,3)` will rerun the cached chunk if the 2nd and 3rd chunks of the document are also rerun. You can also save objects created by a cached chunk to a separate RData file and load that in subsequent chunks with the load command.

### Exercise
Try `cache = TRUE` on the code chunk from earlier. Knit the document. What happens? Run the document again. What happens?



```{r, message=FALSE}
#load packages
library(ggplot2)
library(sf)
library(raster)
library(terra)
library(patchwork)
library(tigris)
library(rnaturalearth)
library(ebirdst)

#Allows to access data in ebirst package
set_ebirdst_access_key("aj6vlciqa1gd", overwrite = TRUE)

# Get the state boundary for Sonora, Mexico
region_boundary <- ne_states(iso_a2 = "MX", returnclass = "sf") |>
  filter(name == "Sonora")

# download data if they haven't already been downloaded
# only weekly 3km relative abundance, median and confidence limits
ebirdst_download_status("ameavo", 
                        pattern = "abundance_(median|upper|lower)_3km")

# load the median weekly relative abundance and lower/upper confidence limits
abd_median <- load_raster("ameavo", product = "abundance", metric = "median")
abd_lower <- load_raster("ameavo", product = "abundance", metric = "lower")
abd_upper <- load_raster("ameavo", product = "abundance", metric = "upper")

# project region boundary to match raster data
region_boundary_proj <- st_transform(region_boundary, st_crs(abd_median))

# extract values within region and calculate the mean
abd_median_region <- extract(abd_median, region_boundary_proj,
                             fun = "mean", na.rm = TRUE, ID = FALSE)
abd_lower_region <- extract(abd_lower, region_boundary_proj,
                            fun = "mean", na.rm = TRUE, ID = FALSE)
abd_upper_region <- extract(abd_upper, region_boundary_proj,
                            fun = "mean", na.rm = TRUE, ID = FALSE)

# transform to data frame format with rows corresponding to weeks
chronology_SO <- data.frame(week = as.Date(names(abd_median)),
                            median = as.numeric(abd_median_region),
                            lower = as.numeric(abd_lower_region),
                            upper = as.numeric(abd_upper_region))


#Make a graph 
AMAV_SO <- ggplot(chronology_SO) +
  aes(x = week, y = median) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "green", alpha = 0.4) +
  geom_line(color = "purple") +
  scale_x_date(date_labels = "%b", date_breaks = "1 month") +
  theme_classic() +
  labs(x = "Week",
       y = "Mean relative abundance in Sonora",
       title = "Migration chronology for American Avocet in Sonora")

AMAV_SO  

```

Here is the code for making the same figure for Idaho:

```{r, message=FALSE}

region_boundary <- ne_states(iso_a2 = "US") |> 
  filter(name == "Idaho")

# download data if they haven't already been downloaded
# only weekly 3km relative abundance, median and confidence limits
ebirdst_download_status("ameavo", 
                        pattern = "abundance_(median|upper|lower)_3km")

# load the median weekly relative abundance and lower/upper confidence limits
abd_median <- load_raster("ameavo", product = "abundance", metric = "median")
abd_lower <- load_raster("ameavo", product = "abundance", metric = "lower")
abd_upper <- load_raster("ameavo", product = "abundance", metric = "upper")

# project region boundary to match raster data
region_boundary_proj <- st_transform(region_boundary, st_crs(abd_median))

# extract values within region and calculate the mean
abd_median_region <- extract(abd_median, region_boundary_proj,
                             fun = "mean", na.rm = TRUE, ID = FALSE)
abd_lower_region <- extract(abd_lower, region_boundary_proj,
                            fun = "mean", na.rm = TRUE, ID = FALSE)
abd_upper_region <- extract(abd_upper, region_boundary_proj,
                            fun = "mean", na.rm = TRUE, ID = FALSE)

# transform to data frame format with rows corresponding to weeks
chronology_ID <- data.frame(week = as.Date(names(abd_median)),
                            median = as.numeric(abd_median_region),
                            lower = as.numeric(abd_lower_region),
                            upper = as.numeric(abd_upper_region))



AMAV_ID <- ggplot(chronology_ID) +
  aes(x = week, y = median) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "green", alpha = 0.4) +
  geom_line(color = "purple") +
  scale_x_date(date_labels = "%b", date_breaks = "1 month") +
  theme_classic() +
  labs(x = "Week",
       y = "Mean relative abundance in Washington",
       title = "Migration chronology for American Avocet in Idaho")

AMAV_ID

```


# References

<div id="refs"></div>


# (APPENDIX) Appendices {-}

# Appendix 1

Citations of all R packages used to generate this report. Reads and prints citations stored in packages.bib

```{r generateBibliography, results = "asis", warning = FALSE, message = FALSE}
### Load R package
library("knitcitations")
### Process and print citations in packages.bib Clear all
### bibliography that could be in the cash
cleanbib()
# Set pandoc as the default output option for bib
options(citation_format = "pandoc")
# Read and print bib from file
read.bibtex(file = "packages.bib")
```

# Appendix 2

Version information about R, the operating system (OS) and attached or R loaded packages. This appendix was generated using `sessionInfo()`.

```{r}
# Load and provide all packages and versions
sessionInfo()
```


