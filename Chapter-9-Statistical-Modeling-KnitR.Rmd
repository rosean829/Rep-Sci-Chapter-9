---
title: "Ch. 9 Statistical Modeling and KnitR"
author: "Rose An, Michael Krzywicki"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: TRUE
    toc_float: TRUE
link-citations: yes
fontsize: 12pt
bibliography: Bibliography_Reproducible_Science_9.bib
csl: AmJBot.csl
---
# Introduction
In this tutorial, we will learn **how to set up a R Markdown document for reproducible statistical analysis/modeling,** primarily using functions and syntax from the *KnitR* package. Please ensure you have the Chapter 9 folder downloaded, as the way this .rmd runs depends on the current file structure.

## Aim
In this tutorial, you will learn methods to ensure your analysis is **reproducible from the moment you start coding to the moment it is published.** You will learn how to **properly set up pipelines** to rerun analysis and present results every time you knit, different methods to **dynamically connect data gathering and source code** to your output documents, and how to keep markdown documents **up to date without overloading your computer** when your analysis is computationally intensive.

## Why is this important?
Coding is hard. It takes a long time, and you'll likely have to go back to the same analysis multiple times to tweak and change things. It is important to **setup a good workflow early** to ensure you're not scrambling to make your research reproducible at the end of the process. You'll be saving yourself a lot of time if your analysis is reproducible and your results presented the way you want them to be **every time you knit your document.**

# Learning Outcomes
Through this tutorial, students will learn to:

1. Analyze data in a code chunk reproducibly.
2. Dynamically report variables in text.
3. Source or code analysis through multiple methods.
4. Ensure results are reproducibly random.
5. Run computationally intensive analysis efficiently.

## Schedule
+ Tuesday: we will ensure that everyone has downloaded the right data and packages, go over code chunk setup, and go over different ways to source/code analysis (LOs 1-3).
+ Thursday: we will learn how to ensure analysis is reproducibly random and how to efficiently run computationally-intensive analysis (LOs 4-5). 


# Packages
Make sure you have the following packages:*"knitr", "rmarkdown", "bookdown", "formattable", "kableExtra", "dplyr", "magrittr", "prettydoc", "htmltools", "knitcitations", "bibtex", "devtools", "tidyverse", "ebirdst", "sf", "raster", "terra", "patchwork", "tigris", "rnaturalearth", "ggplot2"," rnaturalearthhires".* Below are the code chunks for loading packages and setting up the document from Ch. 2:


```{r packages, results = 'hide', message = FALSE, warning = FALSE}
###~~~
# Load R packages
###~~~
#Create a vector w/ the required R packages
# --> If you have a new dependency, don't forget to add it in this vector
pkg <- c("knitr", "rmarkdown", "bookdown", "formattable", "kableExtra", "dplyr", "magrittr", "prettydoc", "htmltools", "knitcitations", "bibtex", "devtools", "tidyverse", "ebirdst", "sf", "raster", "terra", "patchwork", "tigris", "rnaturalearth", "ggplot2", "rnaturalearthhires")

##~~~
#2. Check if pkg are already installed on your computer
##~~~
print("Check if packages are installed")
#This line below outputs a list of packages that are not installed. Which ones of the packages above are installed in computer, print packages that are not installed. Should print 0.
new.pkg <- pkg[!(pkg %in% installed.packages())]

##~~~
#3. Install missing packages
##~~~
# Use an if/else statement to check whether packages have to be installed
# WARNING: If your target R package is not deposited on CRAN then you need to adjust code/function
if(length(new.pkg) > 0){
  print(paste("Install missing package(s):", new.pkg, sep=' '))
  install.packages(new.pkg, dependencies = TRUE)
}else{
  print("All packages are already installed!")
}

##~~~
#4. Load all required packages
##~~~
print("Load packages and return status")
#Here we use the sapply() function to require all the packages
# To know more about the function type ?sapply() in R console
# Just an easier way to load all packages
sapply(pkg, require, character.only = TRUE)
# debug = right-pointing arrow next to code will load in console so you can go line by line and debug. pressing this also seemed to resolve the CRAN needs a mirror error
# Generate BibTex citation file for all loaded R packages
# used to produce report Notice the syntax used here to
# call the function
knitr::write_bib(.packages(), file = "packages.bib")
# The .packages() function returns invisibly the names of all packages loaded in the current R session (to see the output, use .packages(all.available = TRUE)). This ensures that all packages used in your code will have their citation entries written to the .bib file. This packages is then used in the Appendix to cite the code used
```

Setup Chunk Options:
```{r setup, results = 'hide', fig.show='hide', cache = FALSE, message = FALSE}
### Chunk options: see http://yihui.name/knitr/options/ ###
### Text output
opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE, include = TRUE)
## Code formatting
opts_chunk$set(tidy = TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60),
    highlight = TRUE)
## Code caching
opts_chunk$set(cache = 2, cache.path = "cache/")
## Plot output The first dev is the master for the output
## document
opts_chunk$set(fig.path = "Figures_MS/", dev = c("png", "pdf"),
    dpi = 300)
## Figure positioning
opts_chunk$set(fig.pos = "H")
```

# Data
The data for this tutorial should be in the Ch. 9 folder you downloaded. For this chapter, we will be working with a couple of datasets. Mike is working on consolidating some different exploratory analyses he did at different times over the last year. He wants to get them in the same place and make sure that they are reproducible. One set of data comes from the supplementary material of a paper [@Schuetz_and_Johnston_2019]. It is a csv file that was pulled from one of the authors of the paper's github. The other dataset came from the r package `ebirdst`, which interfaces with data from the community science platform eBird. We'll start with a dataset of "encounter rates" calculated from eBird data, which are calculated as the percentage of checklists that include a particular species in a given area. This data has been analyzed for the United States by state, and Mike is getting ready to calculate encounter rates for species in Sonora, Mexico that are also found in Idaho. Before digging into the eBird data for Sonora, Mike wanted to get to know the dataset for Idaho so that he knows what species to look for in the Sonora data. Let's get that data ready. 

## Load Data 
First, please set your working directory to the Chapter 9 folder that you loaded on your computer. You can do this by selecting the working directory through the *session* tab menu, then choosing *to source file location*, or using the command `setwd(*your file path here*`). Now we are going to to do a little bit more data transformation to make a couple of objects of the Idaho encounter rate data grouped taxonomically for some descriptive statistics we will use later.

```{r, fig.show='hide', message = FALSE}

library("tidyverse")

#load csv file of data
state_enc_rate <- read.csv("01_raw_data/state.level.enc.rate.csv")

summary(state_enc_rate)
head(state_enc_rate)

```

There is more data in this dataframe than we need, so lets do some transformation to get the data where we want it. Specifically, we are going to drop a column we don't need, filter the data down to just Idaho, and filter for species with encounter rates higher than 50%. 

```{r, warning = FALSE}
#Get rid of uneccessary columns
state_enc_rate_clean <- state_enc_rate %>%
  dplyr::select(
    -query.volume.state #not needed for analysis
  )

view(state_enc_rate_clean)

#filter to just Idaho
state_enc_rate_clean_filtered <- state_enc_rate_clean %>%
  filter(
    state == "Idaho",
    
  )

view(state_enc_rate_clean_filtered)

#get rid of species with no encounter rate
ID_enc_rate <- state_enc_rate_clean_filtered %>%
  filter(encounter.state.normalized!=0
  )

#get ride of encounter rates below 50 to focus on commonly encou
ID_enc_rate <- ID_enc_rate %>%
  filter(encounter.state.normalized > 50)

#check size of final dataframe
summary(ID_enc_rate)

#make a table for markdown output
kable(ID_enc_rate)

#create clean data folder and save new data file
dir.create("02_Clean_data", showWarnings = FALSE)
write.csv(ID_enc_rate, file = "02_Clean_data/ID_enc_rate.csv",
    row.names = FALSE)

 
```


# Analyze data in a code chunk reproducibly
Although dynamically linking source code to markdown documents is the main goal of this chapter, sometimes it makes sense to include short code chunks directly into your markdown document. There are a variety of customizable options on how the code and its output are displayed in the final knitted document. We use code chunk arguments from the *knitr* package to choose the parts of our code chunks that are outputted in the final html document.

## Set up a chunk's outputs correctly
First, we will learn how to set up a code chunk. We use code chunk options/arguments from the *knitr* package to choose the parts of our code chunks that are outputted in the final html document.

Code chunk options/arguments:

+ `include = FALSE` hides code and results from the output document (html in our case), but code will still run
+ `eval = FALSE` includes code from the document without running it
+ `echo = FALSE` hides code but not results from the document
+ `results = 'hide'` hides results but not code from the document
+ `warning, message, error = FALSE` hides warnings, messages, and error messages from the document
+ `fig.show = 'hide'` hides figures specifically from the final output document

For `eval` and `echo`, you can tell *knitr* which expressions in the chunk to include using numerical vectors. For example, `eval = 1:2` will only include the 1st 2 expressions of code in the output document.

### Thought Excercise:
Take 5 minutes to discuss with the person sitting next to you. Imagine you have an R markdown document with chunks of analysis. You are now sending just the html document outputted from it to your PI for review. What chunk arguments might you set? What if you are publishing the final version of that html document? Now share with the class.

## Analysis in a code chunk
Code chunks are best used when the code is simple or short. To demonstrate analysis in a code chunk for this section, we are going to to do a little bit more data transformation to make a couple of objects of the Idaho encounter rate data grouped taxonomically for some descriptive statistics we will use later. 


```{r, results = 'hide', fig.show='hide', message = FALSE}


#Pull passerines from ID encounter rates list
passerine_enc_rate <- ID_enc_rate %>%
  filter(common.name %in% c("American robin", "bank swallow", "black-billed magpie", "black-headed grosbeak", "Brewer's blackbird", "Bullock's oriole", "Cassin's finch", "Cassin's vireo", "cedar waxwing", "cliff swallow", "dark-eyed junco", "dusky flycatcher", "European starling", "evening grosbeak", "Hammond's flycatcher", "house finch", "lazuli bunting", "MacGillivray's warbler", "mountain chickadee", "northern rough-winged swallow", "olive-sided flycatcher", "pine siskin", "red crossbill", "red-winged blackbird", "sage thrasher", "song sparrow", "western kingbird", "western tanager", "western wood-pewee", "willow flycatcher", "yellow warbler"))

waterfowl_enc_rate <- ID_enc_rate %>%
  filter(common.name %in% c("American coot", "American wigeon", "cinnamon teal", "common goldeneye", "common merganser", "mallard", "ring-necked duck", "western grebe"))

```


# Dynamically report variables in your text
Sometimes you may want to have R code or output appear inline with the rest of the markdown document. These can be static or dynamic.

* **Static**: When you want to display a line of code in the markdown document.
  * **Example**: Here is a line of code from data prep for this lesson: `ID_enc_rate <- state_enc_rate_clean_filtered %>%
  filter(encounter.state.normalized!=0
  )`

- **Dynamic**: When you want to display output of a line of code inline with the rest of the markdown document.
  - **Example**: The average encounter rate for the most commonly encountered passerines in Idaho is `r mean(passerine_enc_rate$encounter.state.normalized, na.rm = TRUE)`.


# Source or code analysis through multiple methods
Bogging down markdown documents with lots of long code chunks can impair readibility. This is where modular analysis comes in. Modular analysis is a way to dynamically included analysis in your .rmd by keeping script for an analysis in a separate file and then linking/calling it in the .rmd. Changing the script file will change the output in the .rmd, and that output can be used for further analysis in the .rmd. Other times where it might be better to link to a script that contains analysis code include: needing the same code for multiple different output documents, making it easier for other researchers to look for specific parts of your analysis. We will go over two ways to dynamically include analysis in your R Markdown document: locally sourced modular analysis and url-sourced modular analysis.  

## Locally sourced modular analysis

We can link to previously written code locally stored on your computer that you want to pull for an analysis. To do so, we will save that code in its own R file and use the `source` command from *knitR* to call it. This method is best utilized in the early stages of your research, as others won't have access to your local files. 

Here is a string of code that was used to produce a figure. This is also made from ebird data, but instead of starting with someone else's caclulations, we are interacting with the package *ebirdst*, which pulls data Cornell's eBird database. 


```{r, message=FALSE, eval = FALSE}
# Note eval = FALSE. This code chunk won't actually run. We are going to use an r file with the same code for this exercise.
#load packages
library(ggplot2)
library(sf)
library(raster)
library(terra)
library(patchwork)
library(tigris)
library(rnaturalearth)
library(ebirdst)

#Allows to access data in ebirst package
set_ebirdst_access_key("aj6vlciqa1gd", overwrite = TRUE)

# Get the state boundary for Sonora, Mexico
region_boundary <- ne_states(iso_a2 = "MX", returnclass = "sf") |>
  filter(name == "Sonora")

# download data if they haven't already been downloaded
# only weekly 3km relative abundance, median and confidence limits
ebirdst_download_status("ameavo", 
                        pattern = "abundance_(median|upper|lower)_3km")

# load the median weekly relative abundance and lower/upper confidence limits
abd_median <- load_raster("ameavo", product = "abundance", metric = "median")
abd_lower <- load_raster("ameavo", product = "abundance", metric = "lower")
abd_upper <- load_raster("ameavo", product = "abundance", metric = "upper")

# project region boundary to match raster data
region_boundary_proj <- st_transform(region_boundary, st_crs(abd_median))

# extract values within region and calculate the mean
abd_median_region <- extract(abd_median, region_boundary_proj,
                             fun = "mean", na.rm = TRUE, ID = FALSE)
abd_lower_region <- extract(abd_lower, region_boundary_proj,
                            fun = "mean", na.rm = TRUE, ID = FALSE)
abd_upper_region <- extract(abd_upper, region_boundary_proj,
                            fun = "mean", na.rm = TRUE, ID = FALSE)

# transform to data frame format with rows corresponding to weeks
chronology_SO <- data.frame(week = as.Date(names(abd_median)),
                            median = as.numeric(abd_median_region),
                            lower = as.numeric(abd_lower_region),
                            upper = as.numeric(abd_upper_region))


#Make a graph 
AMAV_SO <- ggplot(chronology_SO) +
  aes(x = week, y = median) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "green", alpha = 0.4) +
  geom_line(color = "purple") +
  scale_x_date(date_labels = "%b", date_breaks = "1 month") +
  theme_classic() +
  labs(x = "Week",
       y = "Mean relative abundance in Sonora",
       title = "Migration chronology for American Avocet in Sonora")

AMAV_SO  

```

## Exercise
We have put the code from the code chunk above into a separate r file in the Ch. 9 folder. Now we are going to call it with the `source` function. Try running the following code chunk:

```{r}
# Run main analysis
source("03_linked_scripts/Sonora_AMAV_migration_chron.R")

AMAV_SO
```


## Url-sourced modular analysis
Once you are further along in your research, you may want to make sure your file in publicly accessible to ensure reproducibility. You can host your file in a GitHub repository and use the `source_url` command from the *devtools* package to call it.

We have set up a pre-made GitHub file for this section. It contains the following code:

```{r, eval = FALSE}

region_boundary <- ne_states(iso_a2 = "US") |> 
  filter(name == "Idaho")

# download data if they haven't already been downloaded
# only weekly 3km relative abundance, median and confidence limits
ebirdst_download_status("ameavo", 
                        pattern = "abundance_(median|upper|lower)_3km")

# load the median weekly relative abundance and lower/upper confidence limits
abd_median <- load_raster("ameavo", product = "abundance", metric = "median")
abd_lower <- load_raster("ameavo", product = "abundance", metric = "lower")
abd_upper <- load_raster("ameavo", product = "abundance", metric = "upper")

# project region boundary to match raster data
region_boundary_proj <- st_transform(region_boundary, st_crs(abd_median))

# extract values within region and calculate the mean
abd_median_region <- extract(abd_median, region_boundary_proj,
                             fun = "mean", na.rm = TRUE, ID = FALSE)
abd_lower_region <- extract(abd_lower, region_boundary_proj,
                            fun = "mean", na.rm = TRUE, ID = FALSE)
abd_upper_region <- extract(abd_upper, region_boundary_proj,
                            fun = "mean", na.rm = TRUE, ID = FALSE)

# transform to data frame format with rows corresponding to weeks
chronology_ID <- data.frame(week = as.Date(names(abd_median)),
                            median = as.numeric(abd_median_region),
                            lower = as.numeric(abd_lower_region),
                            upper = as.numeric(abd_upper_region))



AMAV_ID <- ggplot(chronology_ID) +
  aes(x = week, y = median) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "green", alpha = 0.4) +
  geom_line(color = "purple") +
  scale_x_date(date_labels = "%b", date_breaks = "1 month") +
  theme_classic() +
  labs(x = "Week",
       y = "Mean relative abundance in Washington",
       title = "Migration chronology for American Avocet in Idaho")

AMAV_ID

```

## Exercise
Use the `source_url` command from the *devtools* package to call the file. Run the following code chunk. 
```{r}


source_url("https://raw.githubusercontent.com/rosean829/Rep-Sci-Chapter-9/main/03_linked_scripts/Idaho_AMAV_migration_chronology.R")

AMAV_ID

```


# Ensure results are reproducibly random
When an analysis produces simulations, a random number generator is generally used so that others can replicate your results. For this tutorial, we will do an exercise to learn the importance of using the `set.seed` command to ensure replicability.

### Exercise:
You and the person next to you use the same distribution variables but diff vs. same `set.seed`. Do you get the same result? What about when you use the same `set.seed`? 

Have the person sitting on the left run the following simple regression with `set.seed(125)`. The person on the right will run the next code chunk. Compare your results. 

```{r, cache=TRUE, message = FALSE}
## The person on the left of each pair will set seed as 125
set.seed(125)

# Draw 1000 numbers
Draw1 <- rnorm(1000, mean = 0, sd = 2) # Summarize Draw1

# Draw 1000 other numbers
Draw2 <- rnorm(1000, mean=5, sd=1.5)

# Plot Draw1 and Draw2
plot(Draw2)

plot(Draw1)

# Create a regression with Draw1 and Draw2, then plot it.
model <- lm(Draw1~Draw2)

plot(model)

## Save the 1st regression in its own Rdata file (this will be relevant in a later section)
save(model, file = "model.RData")
```

The person sitting on the right will run the following code chunk (same code as the chunk above but with `set.seed(150)`). After comparing your results with your partner, run the `set.seed(125)` chunk. Compare your results.
```{r, cache=TRUE, message = FALSE}

## The person on the right of each pair will set seed as 150
# Set seed as 150
set.seed(150)

# Draw 1000 numbers
Draw3 <- rnorm(1000, mean = 0, sd = 2) # Summarize Draw1

# Draw 1000 other numbers
Draw4 <- rnorm(1000, mean=5, sd=1.5)

# Plot both Draw3 and Draw4
plot(Draw3)

plot(Draw4)

# Create a regression from Draw3 and Draw4, then plot it.
model1 <- lm(Draw3~Draw4)

plot(model1)
```


# Run computationally intensive analysis efficiently
When an analysis is too computationally intensive, knowing how to use the cache functions from *knitR* is helpful. To cache a code chunk, use the argument `cache = TRUE`. A cached chunk will only run if the chunk's contents or options change. 

However, subsequent chunks will not be able to access objects created by the cached chunks or any packages loaded in it (all packages should be loaded in their own chunk anyways though), and the cached chunk will not run if a previous chunk with code it depended on changes. You can solve this last problem with the argument `dependson`. A chunk with the `dependson` argument will rerun if the specified chunk is also rerun. To specify a chunk, use a vector of their labels or their number order from the start of the document. For example, `dependson = c(2,3)` will rerun the cached chunk if the 2nd and 3rd chunks of the document are also rerun. 

### Excercise
The 2 code chunks from earlier have the argument `cache = TRUE`. If you haven't knit the document yet, knit it now. If you have, do you remember the load time for the knit? Now that you've knit once, knit the document again. What happens to the knit time?

### Excercise
You can also save objects created by a cached chunk to a separate .RData file and load that in subsequent chunks with the `load` command if you want to keep using the outputs of the cached chunk even when the chunk doesn't run. The last line of code in one of the previous chunks (`save(model, file = "model.RData")`) saved its output in an .RData file. In the next chunk, we will call and then plot the output without running that chunk:

```{r Histogram, message = FALSE}

# Load Sample
load(file = "model.RData" )

summary(model)

plot(model)
```




# References

<div id="refs"></div>


# (APPENDIX) Appendices {-}

# Appendix 1

Citations of all R packages used to generate this report. Reads and prints citations stored in packages.bib

```{r generateBibliography, results = "asis", warning = FALSE, message = FALSE}
### Load R package
library("knitcitations")
### Process and print citations in packages.bib Clear all
### bibliography that could be in the cash
cleanbib()
# Set pandoc as the default output option for bib
options(citation_format = "pandoc")
# Read and print bib from file
read.bibtex(file = "packages.bib")
```

# Appendix 2

Version information about R, the operating system (OS) and attached or R loaded packages. This appendix was generated using `sessionInfo()`.

```{r}
# Load and provide all packages and versions
sessionInfo()
```


